{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Liempieza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_tweets = '../Data/tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\n",
    "    PATH_tweets,\n",
    "    usecols=['id', 'screen_name', 'text'], \n",
    "    index_col='id', \n",
    "    dtype={'screen_name': str, 'text': str}\n",
    "    )\n",
    "tweets = tweets.drop_duplicates(subset='text', keep=False)\n",
    "# tweets = tweets.drop_duplicates(subset=['screen_name', 'text'], keep=False)\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cargamos csv, botamos filas identicas y nulas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def limpiar_retweet(tweet:str):\n",
    "    if tweet.startswith('RT'):\n",
    "        tweet = ''.join(tweet.split(': ')[1:])\n",
    "    return tweet\n",
    "\n",
    "def limpiar_hashtag(tweet:str):\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def limpiar_url(tweet:str):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def limpiar_emoji(tweet:str):\n",
    "    tweet = re.sub(r'\\\\x\\w+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def limpiar_puntuacion(tweet:str):\n",
    "    tweet = re.sub(r'[^\\w\\s@]', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def limpiar_espacios(tweet:str):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def limpiar_mayusculas(tweet:str):\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "def remplazar_tildes(tweet:str):\n",
    "    tweet = tweet.replace('á', 'a')\n",
    "    tweet = tweet.replace('é', 'e')\n",
    "    tweet = tweet.replace('í', 'i')\n",
    "    tweet = tweet.replace('ó', 'o')\n",
    "    tweet = tweet.replace('ú', 'u')\n",
    "    return tweet\n",
    "\n",
    "def limpiar_texto(tweet:str):\n",
    "    tweet = limpiar_retweet(tweet)\n",
    "    tweet = limpiar_hashtag(tweet)\n",
    "    tweet = limpiar_url(tweet)\n",
    "    tweet = limpiar_emoji(tweet)\n",
    "    tweet = limpiar_puntuacion(tweet)\n",
    "    tweet = limpiar_espacios(tweet)\n",
    "    tweet = limpiar_mayusculas(tweet)\n",
    "    tweet = remplazar_tildes(tweet)\n",
    "    tweet = tweet.strip()\n",
    "    return tweet\n",
    "\n",
    "tweets['text'] = tweets.loc[:, 'text'].apply(limpiar_texto)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizamos los tweets, limpiamos los emojis, sacamos los tildes, las mayusculas, los puntos, los urls, retweets y hashtags."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hashing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "def shingles(k, words: np.ndarray) -> list:\n",
    "    temp = []\n",
    "    for i in range(0, len(words)):\n",
    "        temp.append(' '.join(words[i:i + k]))\n",
    "    return list(set(temp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T22:41:56.310930438Z",
     "start_time": "2023-05-30T22:41:56.268950201Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos los shingles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def caracteristica(k:int=2, sample_size:int=None, df:pd.DataFrame=tweets) -> csr_matrix:\n",
    "    if sample_size:\n",
    "        df = df.sample(sample_size)\n",
    "    df['shingles'] = df['text'].str.split().apply(lambda x: np.array(x)).apply(lambda x: shingles(k, x))\n",
    "    shings = df['shingles'].to_numpy()\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    caracteristica = mlb.fit_transform(shings)\n",
    "\n",
    "    return caracteristica"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos una matriz caracteristica en formato sparse de los tweets y shingles, si un shingle esta en un tweet, entonces marcamos ese registro con un 1, si no, con un 0. Esta en formato sparse ya que no tenemos 30 terabytes para poder crear la matriz caracteristica completa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def minhash_vector(sparse_vector:csr_matrix, num_perm:int=128):\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for index in sparse_vector.indices:\n",
    "        minhash.update(str(index).encode('utf8'))\n",
    "    return minhash"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 2\n",
    "s = 0.2\n",
    "threshold = 1 - s\n",
    "n_perm = 128"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = caracteristica(k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lsh = MinHashLSH(threshold=threshold, num_perm=n_perm)\n",
    "hashes = []\n",
    "\n",
    "for i in tqdm(range(matrix.shape[0])):\n",
    "    hashes.append((i, minhash_vector(matrix[i], num_perm=n_perm)))\n",
    "\n",
    "with lsh.insertion_session() as session:\n",
    "    for i, minhash in tqdm(hashes):\n",
    "        session.insert(i, minhash)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usamos los metodos Minhash y MinHashLSH de la libreria datasketch para hashear la matriz caracteristica sparse, ocupamos 128 hashes y la muestra completa deberia hashearse en 20 min aprox. El metodo MinHashLSH ya tine implementado un sistema de querys y el threshold representa la similaridad de jaccard minima que pueden tener dos tweets para ser similares."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos una query para un tweet y persona en especifico, en donde se retorna los tweets similares a este tweet mencionado anteriormente. Los parametros para esto son k = 2 y s = 0.2, tambien creamos un diccionario en donde contamos cuantas veces aparece cada usuario, de esta manera podemos ver que usuario comparte una mayor cantidad de tweets similares con la persona original."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indice = 1007\n",
    "\n",
    "m = minhash_vector(matrix[indice])\n",
    "aprox = lsh.query(m)\n",
    "\n",
    "original = tweets.iloc[indice]\n",
    "print(f'{original[\"screen_name\"] + \";\".ljust(10)} {original[\"text\"]}\\n')\n",
    "\n",
    "repeticiones = {}\n",
    "for i in aprox:\n",
    "    twt = tweets.iloc[i]\n",
    "    print( f'{(str(i) + \",\").ljust(8)} {twt[\"screen_name\"].strip().ljust(20)}:'.ljust(30), twt['text'] )\n",
    "    repeticiones[twt.screen_name] = repeticiones.get(twt.screen_name, 0) + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seleccionamos el usuario que tiene mas tweets similares con el usuario y tweet original de la query, y encontramos todos los tweets de estas dos personas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mas_repetido = max(repeticiones)\n",
    "tweets = pd.read_parquet('../Data/tweets.parquet')\n",
    "par_con_nombres = []\n",
    "par_sin_nombres = []\n",
    "\n",
    "for c, name in enumerate(tweets['screen_name']):\n",
    "    if name == original['screen_name']:\n",
    "        tweet = tweets['text'].iloc[c]\n",
    "        par_con_nombres.append((name, tweet))\n",
    "        par_sin_nombres.append(tweet)\n",
    "    elif name == mas_repetido:\n",
    "        tweet = tweets['text'].iloc[c]\n",
    "        par_con_nombres.append((name, tweet))\n",
    "        par_sin_nombres.append(tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculamos shingles de los tweets hechos por estas dos personas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def shingles(k, words: np.ndarray):\n",
    "    shingles = []\n",
    "    for i in range(0, len(words)):\n",
    "        shing = ' '.join(words[i:i+k])\n",
    "        if len(shing.split()) == k:\n",
    "            shingles.append(shing)\n",
    "    return list(set(shingles))\n",
    "\n",
    "\n",
    "shings_list = []\n",
    "for twt in par_sin_nombres:\n",
    "    twt_shings = shingles(2, np.array(twt.split()))\n",
    "    if twt_shings != []:\n",
    "        for shing in twt_shings:\n",
    "            shings_list.append(shing)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos matriz caracteristica de los shingles y tweets, asignamos un 1 si el shingle se encuentra en el tweet, y un 0 si no."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "def create_characteristic_matrix(list1, list2):\n",
    "    matrix = []\n",
    "    for string1 in list1:\n",
    "        row = []\n",
    "        for string2 in list2:\n",
    "            if string1 in string2:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        matrix.append(row)\n",
    "    return matrix\n",
    "\n",
    "car = create_characteristic_matrix(shings_list, par_sin_nombres)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T22:51:23.053871614Z",
     "start_time": "2023-05-30T22:51:23.038605677Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculamos la similitud de jaccard en la matriz caracteristica, y retorna las posiciones de los tweets similares si la similitud de jaccard entre cada tweet es mayor o igual a un parametro 's'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_jaccard_difference(matrix, s):\n",
    "    num_cols = len(matrix[0])\n",
    "    jaccard_matrix = []\n",
    "\n",
    "    for i in range(num_cols):\n",
    "        for j in range(i + 1, num_cols):\n",
    "            intersection = np.sum(np.logical_and(matrix[:, i], matrix[:, j]))\n",
    "            union = np.sum(np.logical_or(matrix[:, i], matrix[:, j]))\n",
    "\n",
    "            if union != 0:\n",
    "                jaccard = intersection / union\n",
    "                if jaccard >= s:\n",
    "                    jaccard_matrix.append([i, j, jaccard])\n",
    "\n",
    "    return jaccard_matrix\n",
    "\n",
    "posiciones = calculate_jaccard_difference(np.array(car), 0.3)\n",
    "len(posiciones)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T22:53:17.228551212Z",
     "start_time": "2023-05-30T22:53:17.144472732Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente printeamos el par de tweets similares entre las dos personas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IanDiazc: la derecha quiere permanecer en la casa del golpista y el cuatro ojos. \n",
      "z4ncudoelectric: cero esperanza en la derecha tal cual. \n",
      "\n",
      "z4ncudoelectric: con esto queda toda la derecha inhabilitada. \n",
      "IanDiazc: con eso quedan fuera la mitad de la derecha manilarga y raspador a de ollas bien. \n",
      "\n",
      "z4ncudoelectric: con esto queda toda la derecha inhabilitada. \n",
      "IanDiazc: llamen al sr lapiz para que le explique a la derecha del retrazo. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in posiciones:\n",
    "    if par_con_nombres[x[0]][0] != par_con_nombres[x[1]][0]:\n",
    "        # print(par_con_nombres[x[0]], par_con_nombres[x[1]])\n",
    "        par_1 = par_con_nombres[x[0]] ; par_2 = par_con_nombres[x[1]]\n",
    "        print(\n",
    "            f'{par_1[0]}: {par_1[1].strip()}. \\n{par_2[0] + \": \" + par_2[1]}. \\n'\n",
    "        )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T22:53:22.084144780Z",
     "start_time": "2023-05-30T22:53:22.082130544Z"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
